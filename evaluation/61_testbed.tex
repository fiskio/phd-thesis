\section{Experimental Set-up}

This section describes the experimental set-up used in this chapter. We use two testbeds: a local one and
the Emulab testbed, as described in Table~\ref{table:machines}. The local testbed consists of three
machines with 1.8~Ghz~CPUs and 4~GB of memory running Ubuntu Linux 2.6.27-17-server. They are connected
over a 1~Gbps network.
One machine is used as an oracle with a global system view, one for the input sources and query
submission and one as a processing node.
The Emulab testbed consists of a varying number of pc3000-type machines connected over a 1~Gbps LAN network. 
Each machine has a 3~Ghz CPU, 2~GB of memory and runs the FBSD410+RHL90-STD Emulab-configured Linux
image. One machine is used as an oracle, three as input sources and three for the submission of queries.
 
The workloads chosen for the experiments belong to two query classes:  
aggregate (\ie \textnormal{AVG}
%, \textnormal{MAX} 
and \textnormal{COUNT}) and complex (\ie \textnormal{TOP-5} and 
\textnormal{COV}) queries. They are summarised in Table~\ref{table:queries}. 
The queries use a diverse set of operators, namely: \textnormal{average}, 
%\textnormal{max},
\textnormal{top-k}, \textnormal{group-by}, \textnormal{filter}, \textnormal{join}, \textnormal{covariance}, 
\textnormal{time-window}, \textnormal{remote-sender}, \textnormal{remote-receiver} 
and \textnormal{output}. The first query class consists of two aggregate queries, chosen to
investigate the behaviour of the \sic metric under different operator semantics. 
The second class consists of more complex queries used to explore the properties of the \sic metric in
workloads with a broader variety of operators, beyond just the aggregate domain.
These query types are representative for a variety of data processing applications, such as sensor
networks and social media analysis.\\
%% --- TABLE TESTBED ---
\input{evaluation/table_testbed}
%% --- TABLE QUERIES ---
\input{evaluation/table_queries}
%% --- TABLE INPUTS ---
\input{evaluation/table_inputs}

The input data generated by the sources contains either real-world load measurements or a specific
synthetic workload.
The real-world data streams are log traces of CPU load and memory usage measurements from all PlanetLab
nodes~\cite{planetlab} collected in April~2010, as recorded by the CoTop
project~\cite{cotop}.
The values of the synthetic data follow either a \emph{gaussian}, \emph{uniform} or \emph{exponential}
distribution, as described in Table~\ref{table:inputs}.
Furthermore, a \emph{mixed} synthetic workload is used that combines all three distributions randomly.
These synthetic workloads provide controlled experimental input sets that allow for an easier understanding of the results. 
In all experiments, the duration of the source time window~(STW) is set to 10~seconds for all
sources. This value stays well within the variation of processing delays of all the chosen queries.
The shedding interval is set to 250~milliseconds, a value that provides a good trade-off between
throughput and management overhead.

In the experiments using more than one processing node the deployment of
queries emulates the scenario of a federated resource pool, in which the allocation of resources
is not homogeneous. Each query is partitioned into a number of subqueries, and these fragments are
deployed on the available nodes according to a Zipf distribution~\cite{zipf}. Consider, for example,
a scenario in which the number of nodes is 20, divided into 4 clusters of 5 nodes each, with a total
number of query partitions of~150. According to the Zipf law, each cluster is assigned twice as many
subqueries as the previous one. Starting with the deployment of 10 partitions onto the first cluster, the
second receives 20, the third 40 and the fourth 80. 

%% --- TABLE INPUTS ----  
% The \textnormal{AVG}, \textnormal{COUNT} and \textnormal{MAX} aggregate queries connect to one source
% with a rate of 400~tuples/sec, grouped in 5 batches of 80 tuples each.
% The \textnormal{TOP-5} query connects to 20~sources, equally split between CPU load and memory readings.
% Each source has a rate of 20~tuples/sec grouped in 5 batches of 4 each (a total of 400~tuples/sec).
% %The mean for the ten CPU sources is increased from 50 to 230 in steps of 20. 
% Th \textnormal{COV} queries connect to two sources, providing CPU readings with a rate of 200~tuples/sec,
% grouped in 5 batches od 40 tuples each (a total of 400~tuples/sec). 
% 
% The load of 400 tuples/sec was chosen empirically \ldots
%  

%--------------------------------------------------------------------------------------------------------
