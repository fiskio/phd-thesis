In recent years there has been a growing demand for applications capable of processing high-volume data
streams in real-time~\cite{8-reqs}. These stream based applications include, among others, financial
algorithmic trading~\cite{streambase-algo}, environmental monitoring~\cite{swissexp} and the real-time
processing of social media events.  My research focused with the issues arising when building such a
system at extreme scale, able to reliably collect and process data streams from a large number of
sources in a distributed fashion, especially in the case when not enough resources are available to carry
out the requested computation. In this circumstance the system need to gracefully degrade the correctness
of the computed results, following the principle that an approximate result can be better than no result
at all.

I will describe two broad classes of applications that can benefit from this approximate result
approach.  The first comprises research projects aiming at the construction of a \emph{global sensing
infrastructure}~\cite{senseweb,gsn,irisnet}.  These are concerned mostly with sensor data, trying to
develop a common infrastructure in which different classes of sensors can be connected and accessed
through a common interface, allowing users to process data streams generated by different sensor
networks.
 
A second category of applications that can benefit from this approach is the \emph{real-time processing
of social media events}.  These analyse the constant stream of user generated content trying to extract
useful information from it.  It is possible to process user generated data streams, coming for instance
from Twitter, to understand how the public reacts to a certain product or
event~\cite{twitter-sentiment},~\cite{twitrratr}, identifying trends as they
rise~\cite{twitter-stocks}~\cite{twitter-sentiment-1}. It also possible to exploit the locality
information disclosed by the users to identify differences in lifestyle and
preferences~\cite{foursquare-wsj},~\cite{foursquare-rude}.

A particular class of stream processing systems is concerned with \emph{distributed aggreagation}. These
systems reduce the scope of their processing to the computation of aggregate values and are particurlarly
resilient to the loss of input data. I will describe the functioning of such systems, particularly
focusing on the techniques employed to overcome failure and to estimate the achieved quality-of-service
provided~\cite{network-imprecision}.  Finally I will describe some distributed aggregation
strategies which provide good estimates of system wide properties, by employing an approach that mimics
the spread of epidemics.

Then I will present is some more detail the concept of \emph{stream processing}, describing how
these systems operate and what kind of queries they can process. The focus will be on \emph{failure} in
such systems. I will explain the different approaches to \emph{consistency}, or how a system is able to
recover or withstand failure. Finally, I will describe some \emph{replication} techniques taken by different systems to be more resilient to failure.

We consider overload to be a particular kind of failure, as an excessive amount of data can render a
system unable to function.  In this context I will present some techniques employed to reduce the load on
the system, so that it can still function and produce meaningful results.  The first is
\emph{load-shedding}~, which is the deliberate decision to drop a percentage of the incoming
input. Which data is sheddable and where is crucial and can have a great impact on the quality of the
produced results. The second one is \emph{stream reuse}~, which allow the system to multiplex the
same stream of data to various operator, eliminating the redundancy and reducing the memory and
processing required by the given computation.  The third and last technique I am going to present is
based on the \emph{reduction of complexity of operators}~, employing approximate versions which
compute similar results posing a significantly lower computational cost.


