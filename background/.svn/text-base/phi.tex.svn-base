\subsection{Internet Monitoring}
\label{sec:phi}
The second class of stream-based applications, which can successfully operate under partial failure, is large-scale
Internet monitoring. In this case, the data streams are not produced by sensors in the physical world like in the case
of sensor networks. They measure instead, non physical quantities describing events happening within the global network
infrastructure. Internet security is perhaps the most important challenge facing computing today. The spread of malicious
software (viruses, worms, spyware and the like) is constantly growing~\cite{mcafee-report_2q2009}, and comes at a
serious cost for individual users as well as for businesses. 
The arousal of worldwide botnets seems unstoppable, with hundred of thousands of machines
infected everyday, and is a powerful tool to launch DDoS attacks and to spread the ever increasing amount of spam.
It is clear then, how vital it is to the future development of the Internet to be able to monitor the spread of infections,
and in general its overall "health" condition.

\subsubsection*{Public Health fot the Internet ($\varphi$)}
The authors of~\cite{phi} propose a new grand challenge to the computer science community called Public Health for the
Internet. PHI (or $\varphi$) has precisely this objective, to enable machines on the Internet to join forces to establish a
"community health watch" infrastructure that aggressively share, analyses and acts upon information observed at various
machines. The idea is to allow endpoint traffic observations to be shared across machines for the common good.
It is useful to consider the problem in terms of medical analogies. Medicine, as a discipline, is concerned with curing
individuals, and focuses less on widespread problems. The complementary field of Public Health, instead, deals with the
well being of the population as a whole. It is concern in identifying and mitigating the spread of a disease in a
population. So far, Internet security has been perceived in a way which resembles medicine, it has been focused on
individuals or small groups. Firewalls and anti-viruses, even though somehow effective, do not protect and monitor the
community as a whole. The $\varphi$ approach, instead, aims at monitoring the Internet at large, exploiting the information
collaboratively gathered by a large number of monitoring machines.

The idea of being able to monitor the Internet at large is not new. Previously it has been proposed to build a "Center
for Disease Control" for the Internet~\cite{own-the-internet}. The problem with this approach is basic: who runs the
center and who controls the controller? Also, a centralized approach would lack the scalability to be able to
effectively monitor the Internet as a whole. Instead, the $\varphi$ proposal is to establish a global collaborative
infrastructure, able to monitor the Internet at scale. This decentralized architecture very much resembles a
peer-to-peer system, being constituted by a number of independent and hierarchically equivalent monitoring entities.
The $\varphi$ architecture would be composed of at least three main components. First of all, a variety of network
"sensors" software modules, gathering information about the network security and performance at each endpoint. 
The second component would be a peer-to-peer protocol, allowing the dissemination of this information, in order to
enable collaborative analysys of it. Third, there is the need for easy to use, intuitive end-user software interfaces,
both to incent users to join the collective, and to encourage them to be more proactively aware of their online
security.

\paragraph{Stream Processing and \textbf{$\varphi$}}
A key component of the $\varphi$ challange involves distributed, real-time information management. This collectively
gathered security and performance measurements will take the form of distributed streams of data, coupled with
historical repositories. The streams will be generated by a variety of network "sensors", based on log events from
firewalls, operating systems and many other network applications. The amount of streams and produced data is going to be
massive and one of the challange for such a system is to be able to effectively process it. Also, because of the scale
at which the system is designed to operate, it is unfeasible to collect all the data at all times. In particular, since
this application focuses on detecting the spread of infections and attacks, it is very likely that many of the
compromised endpoints won't be able to correctly report their measurements. In fact, partial failure is going to be the
common operational mode for the system. Despite this, though, the infrastructure should be able to identify threats and
to react to them. Having stated these constraints, it is evident how distributed stream processing would be the correct
paradigm to tackle the problem. In particular, this application is ideal for our dependable stream processing system,
which is designed to be massively scalable and able to operate in presence of failure.



%Internet Telescopes \cite{internet-telescope}

