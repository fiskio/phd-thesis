\addcontentsline{toc}{chapter}{Abstract}

\begin{abstract}
\vspace{-15pt}
Data stream processing systems (DSPSs) compute real-time queries over constantly changing streams of
data.  A stream is a possibly infinite sequence of tuples, or timestamped entities.  Differently than
traditional databases, where queries are issued over stored data, in DSPSs queries are first submitted to
the system, and results are generated continuously as the new data enters the system in the form of
streams. This allows the generation of real-time updated results based on the constantly changing
available data streams. \vspace{5pt}\\
% 2
The constant increase in data availability renders the provisioning of a DSPS difficult and costly.
An extremely large amount of input data requires an amount of computing resources that might
not be in availability of the user and too costly to purchase. Even if the user opted for a cloud
 deployment, renting all the resources needed, perfect processing might not be feasible
 due to financial constraints.
For this reasons \emph{overload} should be considered a common operating condition on such systems and
not an exception. \vspace{5pt}\\
%
Many streaming applications are able to produce valuable results even when some failure occurs during the
processing.
Examples of such applications are meso-scale weather prediction, forecasting possibly disruptive events
 like tornadoes and hurricanes, and real-time social media monitoring, used to understand the spread of
 social cascades or the impact of a media campaign as they develop. 
 These can still produce meaningful results, even if some data is lost during 
 the computation since, in many cases, an imperfect result is better than no result at all.
 \vspace{5pt}\\
%
Overload can be considered a kind of failure, since the system is not able to fully carry out the
required computation. A system operating under constant overload is then subject to continuous failure.
In this situations the system needs to discard some of its input data, an operation called
\emph{load~shedding} In such a high load scenario failure, due to overload, is not a transient condition.
Instead the system should be designed taking this into account, implementing approximation strategies 
that allow the system to still produce meaningful results while providing the user with a measure of 
the achieved quality of service. \vspace{5pt}\\
%
We propose a processing model in which the system keeps on operating under failure, constantly estimating
its impact on the computation and reporting to the user the achieved quality-of-service. We introduced
a quality metric called \textit{Information Content}, to augment data streams, and a set of formulas to
calculate its propagation within the system. This has been designed to work for any kind of operator and
for both the \textit{pyramidal} and the \textit{hourglass} families of queries.  
This can be used by the user as an indicator of the quality of service
achieved for their computation. They are also used by the system to self-optimise, reducing the
occurrence of overload and trying to recover from it. \vspace{5pt}\\
%
When an overloaded system performs \textit{load shedding}, the choice of how much and what to discard is
crucial for the correct functioning of the system. The inclusion of the quality metric allows the system
to make a more informed decision when shedding input data. It has the possibility to shed \textit{fairly}
among queries, giving a similar quality of service to all users, without penalising certain kinds of
queries. \vspace{5pt}\\
%
We developed these ideas into a new research prototype called DISSP, the Dependable Internet-Scale Stream
Processing engine. The aim was to design and implement a scalable and dependable stream processing
system, designed to compute massively distributed queries, using data streams generated by a very large
number of sources scattered around the globe.  With it, we explored the issues arising when building a
DSPS at such a scale, with the aim of building a system which is able to operate under constant failure. 
\vspace{5pt}\\
%
We show that augmenting streams with the \emph{Information Content} metric allows the system to make
better decisions about what data to discard, leading to more accurate results for many queries.
It also allows the user to reason about the amount of processing resources that are needed to run a
certain query, giving the mean to strike a balance between the correctness of results and the cost of
operating the system. We show that is still possible to achieve a good approximation of the results
while sensibly reducing the costs for the user.

\end{abstract}
